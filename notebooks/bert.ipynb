{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "単語予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 クリケット\n",
      "1 タイガース\n",
      "2 サッカー\n",
      "3 メッツ\n",
      "4 カブス\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "# Tokenize input\n",
    "text = 'テレビでサッカーの試合を見る。'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "# ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 2\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "# ['テレビ', 'で', '[MASK]', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# [571, 12, 4, 5, 608, 11, 2867, 8]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# tensor([[ 571,   12,    4,    5,  608,   11, 2867,    8]])\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0][0, masked_index].topk(5) # 予測結果の上位5件を抽出\n",
    "\n",
    "# Show results\n",
    "for i, index_t in enumerate(predictions.indices):\n",
    "    index = index_t.item()\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    print(i, token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文予測(まとめて)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 社会\n",
      "1 家\n",
      "2 家庭\n",
      "3 敬\n",
      "4 場\n",
      "\n",
      "\n",
      "0 社会\n",
      "1 家\n",
      "2 家庭\n",
      "3 敬\n",
      "4 場\n",
      "\n",
      "\n",
      "0 社会\n",
      "1 家\n",
      "2 家庭\n",
      "3 身近\n",
      "4 職業\n",
      "\n",
      "\n",
      "0 社会\n",
      "1 家\n",
      "2 家庭\n",
      "3 身近\n",
      "4 的\n",
      "\n",
      "\n",
      "0 社会\n",
      "1 家\n",
      "2 家庭\n",
      "3 身近\n",
      "4 的\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "# Tokenize input\n",
    "text = 'テレビでサッカーの試合を見る。'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "# ['テレビ', 'で', 'サッカー', 'の', '試合', 'を', '見る', '。']\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_indexs = [1,2,3,4,5]\n",
    "# masked_indexs = range(1,len(tokenized_text))\n",
    "for i in masked_indexs:\n",
    "    tokenized_text[i] = '[MASK]'\n",
    "# ['テレビ', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model.eval()\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = [outputs[0][0, i].topk(5) for i in masked_indexs] # 予測結果の上位5件を抽出\n",
    "\n",
    "# Show results\n",
    "for j in range(len(masked_indexs)):\n",
    "    for i, index_t in enumerate(predictions[j].indices):\n",
    "        index = index_t.item()\n",
    "        token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "        print(i, token)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文予測(一つずつ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]テレビでサッカーの試合を見る。[SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "社会\n",
      "[CLS]テレビ社会[MASK][MASK][MASK][MASK][MASK][MASK][SEP]\n",
      "社会\n",
      "[CLS]テレビ社会社会[MASK][MASK][MASK][MASK][MASK][SEP]\n",
      "社会\n",
      "[CLS]テレビ社会社会社会[MASK][MASK][MASK][MASK][SEP]\n",
      "社会\n",
      "[CLS]テレビ社会社会社会社会[MASK][MASK][MASK][SEP]\n",
      "社会\n",
      "[CLS]テレビ社会社会社会社会社会[MASK][MASK][SEP]\n",
      "社会\n",
      "[CLS]テレビ社会社会社会社会社会社会[MASK][SEP]\n",
      "社会\n",
      "[CLS]テレビ社会社会社会社会社会社会社会[SEP]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "# Tokenize input\n",
    "text = 'テレビでサッカーの試合を見る。'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_text.insert(0, '[CLS]')\n",
    "tokenized_text.append('[SEP]')\n",
    "print(''.join(tokenized_text))\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "\n",
    "for itr in range(len(tokenized_text)-3):\n",
    "    # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "    masked_indexs = range(itr+2, len(tokenized_text)-1)\n",
    "    for i in masked_indexs:\n",
    "        tokenized_text[i] = '[MASK]'\n",
    "\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0][0, itr+2].topk(1) # 予測結果の上位1件を抽出\n",
    "\n",
    "    # Show results\n",
    "    index = predictions.indices.item()\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "    tokenized_text[itr+2] = token\n",
    "    print(token)\n",
    "    print(''.join(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc72e38c20544fd8f81d4d5eee68fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536c5907f76248f48fe4ff789d771eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deb77671e5c4dfb8fbd319378d392fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors='pt')\n",
    "loss, logits = model(**encoding, next_sentence_label=torch.LongTensor([1]))\n",
    "assert logits[0, 0] < logits[0, 1] # next sentence was random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109302d2580f4e02989c6440bdceb98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onewood\\Anaconda3\\envs\\bert\\lib\\site-packages\\transformers\\modeling_auto.py:796: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTで穴埋め"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelWithLMHead.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input the biased word\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " soccer player\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The soccer player was asked a question but he didn't answered.\n",
      "The soccer player was asked a question but she didn't answered.\n",
      "The soccer player was asked a question but they didn't answered.\n",
      "The soccer player was asked a question but i didn't answered.\n",
      "The soccer player was asked a question but we didn't answered.\n",
      "The soccer player was asked a question but it didn't answered.\n",
      "The soccer player was asked a question but alex didn't answered.\n",
      "The soccer player was asked a question but adam didn't answered.\n",
      "The soccer player was asked a question but sam didn't answered.\n",
      "The soccer player was asked a question but ryan didn't answered.\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "print(\"input the biased word\")\n",
    "biased_word = input()\n",
    "sentence = \"The {} was asked a question but [MASK] didn't answered.\".format(biased_word)\n",
    "model_input = tokenizer.encode(sentence, return_tensors='pt')\n",
    "mask_token_index = torch.where(model_input == tokenizer.mask_token_id)[1]\n",
    "# predict\n",
    "token_logits = model(model_input)[0]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "#print(\"input the num of prediction\")\n",
    "#k = int(input())\n",
    "k = 10\n",
    "top_k_tokens = torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()\n",
    "for token in top_k_tokens:\n",
    "    print(sentence.replace(\"[MASK]\", tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2で文生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# define model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelWithLMHead.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence which will be followed by He and She\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " The doctor asked the nurse a question. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<predicted sentence>\n",
      "The doctor asked the nurse a question. He asked her if she had ever had a heart attack. She said yes.\n",
      "\n",
      "\"I was just\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<attentions>\n",
      "tensor([0.1297, 0.1377, 0.0685, 0.1729, 0.0456, 0.1963, 0.0608, 0.1189, 0.0697],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.9211, 0.0134, 0.0279, 0.0038, 0.0089, 0.0051, 0.0101, 0.0053, 0.0043],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.9587, 0.0075, 0.0154, 0.0026, 0.0087, 0.0010, 0.0026, 0.0019, 0.0017],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.6217, 0.1098, 0.0158, 0.0038, 0.0832, 0.0099, 0.0820, 0.0173, 0.0566],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.6997, 0.0335, 0.0412, 0.0260, 0.0193, 0.0194, 0.0099, 0.1316, 0.0194],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.8680, 0.0112, 0.0589, 0.0184, 0.0026, 0.0036, 0.0067, 0.0157, 0.0150],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([9.7348e-01, 5.8581e-03, 9.0863e-03, 1.3727e-03, 5.9049e-03, 9.4172e-04,\n",
      "        1.6482e-03, 9.3672e-04, 7.7180e-04], grad_fn=<SelectBackward>)\n",
      "tensor([9.7141e-01, 2.3181e-03, 1.1892e-02, 1.8543e-03, 1.9336e-03, 2.0620e-03,\n",
      "        8.1384e-04, 5.5744e-03, 2.1405e-03], grad_fn=<SelectBackward>)\n",
      "tensor([3.9179e-05, 3.8646e-01, 1.0043e-01, 6.1924e-02, 6.1987e-02, 3.8417e-02,\n",
      "        2.2991e-02, 6.4978e-02, 2.6277e-01], grad_fn=<SelectBackward>)\n",
      "tensor([0.9168, 0.0048, 0.0306, 0.0104, 0.0017, 0.0051, 0.0132, 0.0135, 0.0039],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.6466, 0.0797, 0.0772, 0.0134, 0.0262, 0.0251, 0.0398, 0.0433, 0.0487],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.8328, 0.0491, 0.0209, 0.0051, 0.0226, 0.0088, 0.0317, 0.0040, 0.0251],\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n",
      "\n",
      "<predicted sentence>\n",
      "The doctor asked the nurse a question. She said, \"I'm not sure what you're talking about.\"\n",
      "\n",
      "\"I'm not sure what\n",
      "<attentions>\n",
      "tensor([0.1060, 0.1088, 0.0644, 0.1771, 0.0422, 0.2367, 0.0657, 0.1101, 0.0890],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.8960, 0.0101, 0.0233, 0.0041, 0.0077, 0.0067, 0.0074, 0.0408, 0.0040],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.9600, 0.0064, 0.0164, 0.0025, 0.0078, 0.0011, 0.0025, 0.0021, 0.0012],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.6256, 0.1207, 0.0149, 0.0033, 0.1012, 0.0085, 0.0735, 0.0176, 0.0346],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.7272, 0.0292, 0.0369, 0.0237, 0.0170, 0.0156, 0.0079, 0.1234, 0.0190],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.8448, 0.0089, 0.0571, 0.0235, 0.0027, 0.0039, 0.0065, 0.0263, 0.0263],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([9.7228e-01, 5.5474e-03, 8.5315e-03, 1.2624e-03, 8.3616e-03, 7.7289e-04,\n",
      "        1.7338e-03, 8.3993e-04, 6.6987e-04], grad_fn=<SelectBackward>)\n",
      "tensor([0.9691, 0.0028, 0.0111, 0.0017, 0.0036, 0.0018, 0.0011, 0.0068, 0.0020],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([6.9147e-05, 3.6419e-01, 9.7157e-02, 3.9886e-02, 6.4989e-02, 2.7880e-02,\n",
      "        1.9739e-02, 4.2030e-02, 3.4406e-01], grad_fn=<SelectBackward>)\n",
      "tensor([0.9177, 0.0033, 0.0234, 0.0101, 0.0015, 0.0040, 0.0074, 0.0277, 0.0049],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.6128, 0.0970, 0.0811, 0.0121, 0.0336, 0.0167, 0.0449, 0.0501, 0.0517],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.7799, 0.0630, 0.0247, 0.0057, 0.0431, 0.0094, 0.0372, 0.0044, 0.0327],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"input sentence which will be followed by He and She\")\n",
    "sentence = input()\n",
    "\n",
    "# preprocess\n",
    "male_sentence = sentence + \"He\"\n",
    "male_model_input = tokenizer(male_sentence, return_tensors='pt')\n",
    "# predict\n",
    "male_output = model.generate(male_model_input['input_ids'], max_length=30)\n",
    "print(\"<predicted sentence>\")\n",
    "print(tokenizer.decode(male_output[0]))\n",
    "male_last_attention = model(**male_model_input, output_attentions=True)[2][-1]\n",
    "print(\"<attentions>\")\n",
    "for i in range(12):\n",
    "    print(male_last_attention[0][i][8])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# preprocess\n",
    "female_sentence = sentence + \"She\"\n",
    "female_model_input = tokenizer(female_sentence, return_tensors='pt')\n",
    "# predict\n",
    "female_output = model.generate(female_model_input['input_ids'], max_length=30)\n",
    "print(\"<predicted sentence>\")\n",
    "print(tokenizer.decode(female_output[0]))\n",
    "female_last_attention = model(**female_model_input, output_attentions=True)[2][-1]\n",
    "print(\"<attentions>\")\n",
    "for i in range(12):\n",
    "    print(female_last_attention[0][i][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get occupations from dataset\n",
    "def isEnglish(word):\n",
    "    flag = True\n",
    "    for i in word:\n",
    "        if ord(i) > 127:\n",
    "            flag = False\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "occupations = []\n",
    "with open('data/occupations.wikidata.all.labeled.tsv', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        occupation = line.split(\"\\t\")[2]\n",
    "        if isEnglish(occupation):\n",
    "            occupations.append(line.split(\"\\t\")[2])\n",
    "occupations = occupations[1:]\n",
    "with open('data/occupations.txt', encoding=\"utf-8\", mode='w') as f:\n",
    "    for i in occupations:\n",
    "        f.write(i+\"\\n\")\n",
    "    f.write(\"職業数：{}\".format(len(occupations)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
